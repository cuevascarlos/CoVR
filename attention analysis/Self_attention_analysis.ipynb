{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Attention matrices analysis\n",
        "\n",
        "Authors: Carlos CUEVAS VILLARMIN and Javier Alejandro LOPETEGUI GONZALEZ\n",
        "\n",
        "This notebook has been written under the final project for the course Object Recognition and Computer Vision in the MVA master program.\n",
        "\n",
        "The purpose of the notebook is gain insights about what the model CoVR-BLIP-2 has learned based on the attention matrices over layers and heads. Furthermore, we propose a way of weighting the learnable queries and modification text tokens embeddings based on their relevance considering the attention outputs.\n",
        "\n",
        "**Remark:** To use this notebook it is necessary to have the attentions for test set downloaded. The examples used are the ones reported in the report of the project."
      ],
      "metadata": {
        "id": "L46A-LwCwQGz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4-yZr4NxJt21"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/', force_remount=True)\n",
        "!mkdir -p \"/content/datasets/CIRR\"\n",
        "!ln -s \"/content/drive/MyDrive/path-to-images\" \"/content/datasets/CIRR/images\"\n",
        "!ln -s \"/content/drive/MyDrive/path-to-attentions\" \"/content/datasets/CIRR/attentions\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "\n",
        "# Load .pt file\n",
        "att_directory = \"/content/datasets/CIRR/attentions\"\n",
        "pt_files = [f for f in os.listdir(att_directory) if re.match(r'^attentions_\\d+\\.pt$', f)]\n",
        "pt_file_path = os.path.join(att_directory, pt_files[0])\n",
        "all_attention_outputs = []\n",
        "\n",
        "for _, file in tqdm(enumerate(pt_files)):\n",
        "  # Load the file content (which is a list)\n",
        "  attention_outputs_list = torch.load(os.path.join(att_directory, file))\n",
        "  attention_outputs = torch.stack(attention_outputs_list)\n",
        "  all_attention_outputs.append(attention_outputs)\n",
        "\n",
        "# Concatenate all tensors in the list\n",
        "concatenated_attention_outputs = torch.stack(all_attention_outputs)\n",
        "# Verify the content\n",
        "print(len(attention_outputs_list))\n",
        "print(len(all_attention_outputs))\n",
        "print(type(concatenated_attention_outputs))\n",
        "print(concatenated_attention_outputs.shape)"
      ],
      "metadata": {
        "id": "fdrKRTjaKXNa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read captions and IDS\n",
        "import json\n",
        "\n",
        "with open('/content/datasets/CIRR/attentions/captions_and_ids.json', 'r') as f:\n",
        "    file_dict = json.load(f)\n",
        "\n",
        "captions = file_dict['caption']\n",
        "ids = file_dict['pair_id']"
      ],
      "metadata": {
        "id": "tv1FPMwpZ62L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load pre-trained BERT tokenizer\n",
        "tokenizer = tokenizer = BertTokenizer.from_pretrained(\n",
        "            \"bert-base-uncased\", truncation_side=\"right\"\n",
        "        )\n",
        "tokenizer.add_special_tokens({\"bos_token\": \"[DEC]\"})"
      ],
      "metadata": {
        "id": "B4sKjrMpZim6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def get_input_ids_and_tokens(tokenizer, caption, print_flag=False):\n",
        "  text_tokens = tokenizer(\n",
        "                  caption,\n",
        "                  padding=\"longest\",\n",
        "                  truncation=True,\n",
        "                  max_length=64,\n",
        "                  return_tensors=\"pt\",\n",
        "              )\n",
        "\n",
        "  input_ids = text_tokens['input_ids'][0]\n",
        "  # Convert input_ids to tokens\n",
        "  tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "\n",
        "  if print_flag:\n",
        "    # Create a pandas DataFrame for the table\n",
        "    df = pd.DataFrame({\"Input IDs\": input_ids, \"Tokens\": tokens})\n",
        "\n",
        "    # Print the DataFrame as a table\n",
        "    print(df.to_string())\n",
        "\n",
        "  return input_ids, tokens"
      ],
      "metadata": {
        "id": "eILXBFlUaqvk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_samples = [0, 5, 17, 90]\n",
        "\n",
        "for sample in batch_samples:\n",
        "    _, tokens = get_input_ids_and_tokens(tokenizer, captions[sample], print_flag = True)"
      ],
      "metadata": {
        "id": "MZzjb-OclVe9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_heatmap(attention, tokens, path_to_save, layer_idx, name):\n",
        "    '''\n",
        "    Function to plot a heatmap of attention\n",
        "\n",
        "    Args:\n",
        "      attention: [seq_len, seq_len]\n",
        "      tokens: list of tokens\n",
        "      name: name of the heatmap\n",
        "      path_to_save: path to save the heatmap\n",
        "      layer_idx: index of the layer\n",
        "    '''\n",
        "    # Plot the heatmap\n",
        "    fig_avg, ax_avg = plt.subplots(figsize=(10, 10))\n",
        "    sns.heatmap(attention, cmap=\"hot\", ax=ax_avg, cbar=False, vmin=0, vmax=1)\n",
        "    ax_avg.set_title(name, fontsize=20)\n",
        "    ax_avg.tick_params(left=False, bottom=False)\n",
        "    ax_avg.set_xticks(np.arange(63))  # Set tick positions for 63 elements\n",
        "    ax_avg.set_xticklabels([\"\"] * 32 + tokens + [\"[PAD]\"] * (63 - 32 - len(tokens)), rotation=45, ha=\"right\", fontsize = 8)\n",
        "    ax_avg.set_yticks(np.arange(63))  # Set tick positions for 63 elements\n",
        "    ax_avg.set_yticklabels([\"\"] * 32 + tokens + [\"[PAD]\"] * (63 - 32 - len(tokens)), rotation=0, va=\"center\", fontsize = 8)\n",
        "    ax_avg.set_xlabel(\"Keys\", fontsize=12)\n",
        "    ax_avg.set_ylabel(\"Queries\", fontsize=12)\n",
        "    fig_avg.savefig(f\"{path_to_save}/{name}.png\", bbox_inches='tight', pad_inches=0)\n",
        "    fig_avg.savefig(f\"{path_to_save}/{name}.pdf\", bbox_inches='tight', pad_inches=0, format='pdf')\n",
        "    plt.close(fig_avg)\n",
        "    #plt.tight_layout()\n",
        "    #plt.show()"
      ],
      "metadata": {
        "id": "-8ctJKkLiI6m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Attention matrices layer by layer and head by head"
      ],
      "metadata": {
        "id": "aV7yy3Lijt_D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "def plot_attention_layers(concatenated_attention_outputs, BATCH_INDEX, BATCH_SAMPLE):\n",
        "    '''\n",
        "    Function to plot the attention matrices for a given batch and sample\n",
        "\n",
        "    Args:\n",
        "      concatenated_attention_outputs: [num_batches, num_layers, batch_size, num_heads, seq_len, seq_len]\n",
        "      BATCH_INDEX: index of the batch\n",
        "      BATCH_SAMPLE: index of the sample\n",
        "    '''\n",
        "    path_to_save = f\"/content/drive/MyDrive/CoVR-FP/figures/attentions_heatmaps/batchID_{BATCH_INDEX}-batchSAMPLE_{BATCH_SAMPLE}\"\n",
        "    os.makedirs(path_to_save, exist_ok=True)\n",
        "\n",
        "    # Get the tokens of the sample\n",
        "    input_ids, tokens = get_input_ids_and_tokens(tokenizer, captions[BATCH_SAMPLE])\n",
        "\n",
        "    # Get the dimensions for the subplot grid\n",
        "    num_layers = len(concatenated_attention_outputs[0])\n",
        "    num_heads = concatenated_attention_outputs[0][0].shape[1]\n",
        "\n",
        "    # Create the figure and subplots\n",
        "    fig_global, axes_global = plt.subplots(num_layers, num_heads, figsize=(num_heads * 10, num_layers * 10))\n",
        "    #fig_global.suptitle(\"Attentions\", fontsize=30)\n",
        "\n",
        "    # Iterate through layers and heads to plot heatmaps\n",
        "    for layer_idx in range(num_layers):\n",
        "\n",
        "        # Get the avg head attention per layer\n",
        "        avg_head_attention = torch.mean(concatenated_attention_outputs[BATCH_INDEX, layer_idx, BATCH_SAMPLE], dim=0).cpu().detach().numpy()\n",
        "        plot_heatmap(avg_head_attention, tokens, path_to_save, layer_idx, f\"Avg Attention Matrix - Layer {layer_idx+1}\")\n",
        "\n",
        "        # Create the figure and subplots\n",
        "        fig, axes = plt.subplots(1, num_heads, figsize=(num_heads*10, 10))  # Adjust figsize as needed\n",
        "        #fig.suptitle(f\"Layer {layer_idx + 1}\", fontsize=20)  # Add a title for the layer\n",
        "\n",
        "\n",
        "        for head_idx in range(num_heads):\n",
        "            print(f\"Computing layer {layer_idx+1} and head {head_idx+1}\")\n",
        "            # Get attention weights for the current layer and head\n",
        "            head_attention = concatenated_attention_outputs[BATCH_INDEX][layer_idx, :, head_idx, :].cpu().detach().numpy()\n",
        "            plot_heatmap(head_attention[BATCH_SAMPLE], tokens, path_to_save, layer_idx, f\"Attention Matrix - Layer {layer_idx + 1} - Head {head_idx + 1}\")\n",
        "\n",
        "            # Plot the heatmap on the corresponding subplot\n",
        "            sns.heatmap(head_attention[BATCH_SAMPLE], cmap=\"hot\", ax=axes[head_idx], cbar=False, vmin=0, vmax=1)\n",
        "            #axes[head_idx].set_title(f\"Head {head_idx + 1}\", fontsize=8)\n",
        "            axes[head_idx].tick_params(left=False, bottom=False)\n",
        "            axes[head_idx].set_xticks([])\n",
        "            axes[head_idx].set_yticks([])\n",
        "\n",
        "            # Add to the main figure\n",
        "            axes_global[layer_idx, head_idx].imshow(head_attention[BATCH_SAMPLE], cmap=\"hot\", aspect=\"auto\", vmin=0, vmax=1)\n",
        "            axes_global[layer_idx, head_idx].tick_params(left=False, bottom=False)\n",
        "            axes_global[layer_idx, head_idx].set_xticks([])\n",
        "            axes_global[layer_idx, head_idx].set_yticks([])\n",
        "\n",
        "        # Adjust layout and display the figure\n",
        "        plt.tight_layout()\n",
        "        fig.savefig(f\"{path_to_save}/heatmap_layer_{layer_idx + 1}.png\")\n",
        "        fig.savefig(f\"{path_to_save}/heatmap_layer_{layer_idx + 1}.pdf\", bbox_inches='tight', pad_inches=0, format='pdf')\n",
        "        plt.show()\n",
        "        plt.close(fig)\n",
        "\n",
        "\n",
        "\n",
        "    # Adjust layout and display the figure\n",
        "    plt.tight_layout()\n",
        "    fig_global.savefig(f\"{path_to_save}/all_layers_heads_heatmaps.png\")\n",
        "    fig_global.savefig(f\"{path_to_save}/all_layers_heads_heatmaps.pdf\", bbox_inches='tight', pad_inches=0, format='pdf')\n",
        "    plt.show()\n",
        "    plt.close(fig_global)\n"
      ],
      "metadata": {
        "id": "w0x6t4psLMm9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_samples = [0, 5, 17, 90]\n",
        "\n",
        "for sample in batch_samples:\n",
        "    plot_attention_layers(concatenated_attention_outputs, BATCH_INDEX=0, BATCH_SAMPLE=sample)"
      ],
      "metadata": {
        "id": "5lAbqcb-lbXA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Attention rollout matrices layer by layer and head by head"
      ],
      "metadata": {
        "id": "4qxh3fO9jzh3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def attention_rollout(As):\n",
        "    \"\"\"\n",
        "    Computes attention rollout from the given list of attention matrices.\n",
        "    https://arxiv.org/abs/2005.00928\n",
        "\n",
        "    Args:\n",
        "      As: Attention matrices [num_layers, num_heads, seq_len, seq_len]\n",
        "\n",
        "    Returns:\n",
        "      rollout: Attention rollout matrices [num_layers, num_heads, seq_len, seq_len]\n",
        "    \"\"\"\n",
        "    num_layers = As.shape[0]\n",
        "    seq_len = As.shape[-1]\n",
        "    rollout = [None] * num_layers\n",
        "\n",
        "    for i in range(num_layers):\n",
        "        if i == 0:  # Base case\n",
        "            # Add residual connection and normalize\n",
        "            rollout[i] = 0.5 * As[i] + 0.5 * torch.eye(seq_len).to(As.device)\n",
        "        else:\n",
        "            # Add residual connection, normalize, and multiply with the previous layer's rollout\n",
        "            current = 0.5 * As[i] + 0.5 * torch.eye(seq_len).to(As.device)\n",
        "            rollout[i] = torch.matmul(current, rollout[i - 1])\n",
        "\n",
        "    rollout = torch.stack(rollout)  # Stack all rollouts layer by layer\n",
        "    return rollout\n",
        "\n",
        "def attention_rollout_per_sample(attentions, BATCH_INDEX, BATCH_SAMPLE):\n",
        "    '''\n",
        "    Function to compute the attention rollout for a given batch and sample\n",
        "\n",
        "    Args:\n",
        "      attentions: [num_batches, num_layers, batch_size, num_heads, seq_len, seq_len]\n",
        "      BATCH_INDEX: index of the batch\n",
        "      BATCH_SAMPLE: index of the sample\n",
        "    '''\n",
        "    # Select the attention for the specified batch and the first layer\n",
        "    attention = attentions[BATCH_INDEX, :, BATCH_SAMPLE]  # Shape: [num_layers, num_heads, seq_len, seq_len]\n",
        "    rollout = attention_rollout(attention)\n",
        "    return rollout\n",
        "\n",
        "\n",
        "def plot_attention_rollout(attentions, BATCH_INDEX, BATCH_SAMPLE):\n",
        "    '''\n",
        "    Function to plot the attention rollout matrices for a given batch and sample\n",
        "\n",
        "    Args:\n",
        "      concatenated_attention_outputs: [num_batches, num_layers, batch_size, num_heads, seq_len, seq_len]\n",
        "      BATCH_INDEX: index of the batch\n",
        "      BATCH_SAMPLE: index of the sample\n",
        "    '''\n",
        "\n",
        "    path_to_save = f\"/content/drive/MyDrive/CoVR-FP/figures/attentions_heatmaps/batchID_{BATCH_INDEX}-batchSAMPLE_{BATCH_SAMPLE}/attention_rollout\"\n",
        "    os.makedirs(path_to_save, exist_ok=True)\n",
        "\n",
        "    # Get the tokens of the sample\n",
        "    input_ids, tokens = get_input_ids_and_tokens(tokenizer, captions[BATCH_SAMPLE])\n",
        "    # Get the attention rollout\n",
        "    rolled_out_attention = attention_rollout_per_sample(concatenated_attention_outputs, BATCH_INDEX, BATCH_SAMPLE)  # Dim: [num_layer, num_head, seq_len, seq_len]\n",
        "    #rolled_out_attention_mean = torch.mean(rolled_out_attention, dim=1).cpu().detach().numpy()  # Dim: [num_layer, seq_len, seq_len]\n",
        "    num_layers = rolled_out_attention.shape[0]\n",
        "    num_heads = rolled_out_attention.shape[1]\n",
        "\n",
        "    # Create the figure and subplots\n",
        "    fig_global, axes_global = plt.subplots(num_layers, num_heads, figsize=(num_heads * 10, num_layers * 10))\n",
        "    #fig_global.suptitle(\"Attention Rollout\", fontsize=30)\n",
        "\n",
        "    for layer_idx in range(num_layers):\n",
        "        # Get the avg head attention per layer\n",
        "        avg_rollout_attention = torch.mean(rolled_out_attention[layer_idx], dim=0).cpu().detach().numpy()\n",
        "        plot_heatmap(avg_rollout_attention, tokens, path_to_save, layer_idx, f\"Avg Attention Rollout - Layer {layer_idx+1}\")\n",
        "\n",
        "        # Create the figure and subplots\n",
        "        fig_layer, axes_layer = plt.subplots(1, num_heads, figsize=(num_heads*10, 10))  # Adjust figsize as needed\n",
        "        #fig_layer.suptitle(f\"Layer {layer_idx + 1}\", fontsize=20)  # Add a title for the layer\n",
        "\n",
        "        for head_idx in range(num_heads):\n",
        "          fig, ax = plt.subplots(figsize=(10, 10))\n",
        "          plot_heatmap(rolled_out_attention[layer_idx,head_idx].cpu().detach().numpy(), tokens, path_to_save, layer_idx, f\"Attention Rollout - Layer {layer_idx + 1} - Head {head_idx + 1}\")\n",
        "\n",
        "          #Add to the global figure\n",
        "          axes_global[layer_idx, head_idx].imshow(rolled_out_attention[layer_idx,head_idx].cpu().detach().numpy(), cmap=\"hot\", aspect=\"auto\", vmin=0, vmax=1)\n",
        "          axes_global[layer_idx, head_idx].tick_params(left=False, bottom=False)\n",
        "          axes_global[layer_idx, head_idx].set_xticks([])\n",
        "          axes_global[layer_idx, head_idx].set_yticks([])\n",
        "\n",
        "          #Add to the layer figure\n",
        "          axes_layer[head_idx].imshow(rolled_out_attention[layer_idx,head_idx].cpu().detach().numpy(), cmap=\"hot\", aspect=\"auto\", vmin=0, vmax=1)\n",
        "          axes_layer[head_idx].tick_params(left=False, bottom=False)\n",
        "          axes_layer[head_idx].set_xticks([])\n",
        "          axes_layer[head_idx].set_yticks([])\n",
        "\n",
        "        #Save fig_layer\n",
        "        fig_layer.savefig(f\"{path_to_save}/heatmap_rollout_layer_{layer_idx+1}.png\", bbox_inches='tight', pad_inches=0)\n",
        "        fig_layer.savefig(f\"{path_to_save}/heatmap_rollout_layer_{layer_idx+1}.pdf\", bbox_inches='tight', pad_inches=0, format='pdf')\n",
        "        plt.close(fig_layer)\n",
        "\n",
        "    #Save global\n",
        "    fig_global.savefig(f\"{path_to_save}/all_layers_heads_attention_rollout.png\", bbox_inches='tight', pad_inches=0, format='pdf')\n",
        "    fig_global.savefig(f\"{path_to_save}/all_layers_heads_attention_rollout.pdf\", bbox_inches='tight', pad_inches=0, format='pdf')\n",
        "    plt.close(fig_global)\n"
      ],
      "metadata": {
        "id": "aX5EsZfrzFqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_samples = [0, 5, 17, 90]\n",
        "\n",
        "for sample in batch_samples:\n",
        "    plot_attention_rollout(concatenated_attention_outputs, BATCH_INDEX=0, BATCH_SAMPLE=sample)"
      ],
      "metadata": {
        "id": "tzDU8DZFle4D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Attention rollout weights apporach"
      ],
      "metadata": {
        "id": "3JFcQvmjkMjk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn import functional as F\n",
        "\n",
        "def compute_weights_based_on_attention_rollout(attentions, BATCH_INDEX, BATCH_SAMPLE, temp=1, return_logs = False):\n",
        "  '''\n",
        "  Compute the weights based on the attention rollout\n",
        "\n",
        "  Args:\n",
        "    attentions: [num_batches, num_layers, batch_size, num_heads, seq_len, seq_len]\n",
        "    BATCH_INDEX: index of the batch\n",
        "    BATCH_SAMPLE: index of the sample\n",
        "\n",
        "  Returns:\n",
        "    queries_dist: distribution/weights of the learnable queries\n",
        "    text_dist: distribution/weights of the modification text tokens\n",
        "  '''\n",
        "  rollout = attention_rollout_per_sample(attentions, BATCH_INDEX, BATCH_SAMPLE) #Dim: [num_layers, num_heads, seq_len, seq_len]\n",
        "  # Average over last layer\n",
        "  rollout_avg_last_layer = torch.mean(rollout[-1], dim=0) # Dim: [seq_len, seq_len]\n",
        "\n",
        "  # Get the average for each column\n",
        "  rollout_avg_last_layer_per_column = torch.mean(rollout_avg_last_layer, dim=0) # Dim: [seq_len]\n",
        "\n",
        "  #Split into values for queries and text\n",
        "  queries_rollout = rollout_avg_last_layer_per_column[:32]\n",
        "  text_rollout    = rollout_avg_last_layer_per_column[32:]\n",
        "\n",
        "  # Set to low value to all the values that are under a threshold\n",
        "  threshold = 0.01\n",
        "  queries_rollout[queries_rollout < threshold] = -1e7\n",
        "  text_rollout[text_rollout < threshold] = -1e7\n",
        "\n",
        "  #How many non-zero values are\n",
        "  if return_logs:\n",
        "    print(f\"Queries rollouts lower than 0: {torch.sum(queries_rollout < 0)} out of {len(queries_rollout)}\")\n",
        "    print(f\"Text rollouts lower than 0: {torch.sum(text_rollout < 0)} out of {len(text_rollout)}\")\n",
        "\n",
        "  queries_dist = F.softmax(queries_rollout/temp, dim=0)\n",
        "  text_dist = F.softmax(text_rollout/temp, dim=0)\n",
        "\n",
        "  return queries_dist, text_dist\n",
        "\n"
      ],
      "metadata": {
        "id": "J6BwYH8HDqMe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_interesting_text_tokens(text_dist, tokenizer, BATCH_SAMPLE):\n",
        "  '''\n",
        "  Function to get the interesting text tokens\n",
        "\n",
        "  Args:\n",
        "    text_dist: distribution/weights of the modification text tokens\n",
        "    tokenizer: tokenizer\n",
        "    BATCH_SAMPLE: index of the sample\n",
        "  '''\n",
        "  _, tokens = get_input_ids_and_tokens(tokenizer, captions[BATCH_SAMPLE])\n",
        "\n",
        "  #Get the ids of the dist > 0\n",
        "  ids = torch.where(text_dist > 0)[0]\n",
        "  for id in ids:\n",
        "    try:\n",
        "      print(f\"Token: {tokens[id]} \\t - Prob: {text_dist[id]:.4f}\")\n",
        "    except IndexError:\n",
        "      print(f\"Token: [PAD] \\t - Prob: {text_dist[id]:.4f}\")"
      ],
      "metadata": {
        "id": "NujKiAsqHiqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_samples = [0, 5, 17, 90]\n",
        "\n",
        "for sample in batch_samples:\n",
        "    _, text_dist = compute_weights_based_on_attention_rollout(concatenated_attention_outputs, BATCH_INDEX=0, BATCH_SAMPLE=sample)\n",
        "    get_interesting_text_tokens(text_dist, tokenizer, BATCH_SAMPLE=sample)"
      ],
      "metadata": {
        "id": "5Hf-NohbIGn2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For all the samples in all the batches\n",
        "for i in range(len(concatenated_attention_outputs)):\n",
        "  for j in range(concatenated_attention_outputs.shape[2]):\n",
        "    print(f\"Pair id: {ids[j]}\")\n",
        "    queries_dist, text_dist = compute_weights_based_on_attention_rollout(concatenated_attention_outputs, i, j, temp=0.2, return_logs=True)\n",
        "    get_interesting_text_tokens(text_dist, tokenizer, j)\n",
        "    #print(queries_dist)"
      ],
      "metadata": {
        "id": "6nxrau2EFWWZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "def plot_attention_rollout_per_query(attentions, BATCH_INDEX, BATCH_SAMPLE):\n",
        "    '''\n",
        "    Create the attention rollout evolution over layers for each query\n",
        "\n",
        "    Args:\n",
        "      concatenated_attention_outputs: [num_batches, num_layers, batch_size, num_heads, seq_len, seq_len]\n",
        "      BATCH_INDEX: index of the batch\n",
        "      BATCH_SAMPLE: index of the sample\n",
        "    '''\n",
        "    path_to_save = f\"/content/drive/MyDrive/CoVR-FP/figures/attentions_heatmaps/batchID_{BATCH_INDEX}-batchSAMPLE_{BATCH_SAMPLE}/query_evol_rollout\"\n",
        "    os.makedirs(path_to_save, exist_ok=True)\n",
        "\n",
        "\n",
        "    input_ids, tokens = get_input_ids_and_tokens(tokenizer, captions[BATCH_SAMPLE])\n",
        "\n",
        "\n",
        "    rolled_out_attention = attention_rollout_per_sample(concatenated_attention_outputs, BATCH_INDEX, BATCH_SAMPLE)  # Dim: [num_layer, num_head, seq_len, seq_len]\n",
        "    num_layers = rolled_out_attention.shape[0]\n",
        "    num_heads = rolled_out_attention.shape[1]\n",
        "    seq_len = rolled_out_attention.shape[3]\n",
        "\n",
        "\n",
        "    for query_idx in tqdm(range(seq_len)):\n",
        "        for head_idx in range(num_heads):\n",
        "            query_head_evolution = []\n",
        "            for layer_idx in range(num_layers):\n",
        "                head_attention = rolled_out_attention[layer_idx, head_idx].cpu().detach().numpy()  # [seq_len, seq_len]\n",
        "                query_head_evolution.insert(0, head_attention[query_idx])\n",
        "\n",
        "            query_head_evolution = np.array(query_head_evolution)  # [num_layers, seq_len]\n",
        "\n",
        "            fig, ax = plt.subplots(figsize=(12,8))\n",
        "            cax = ax.imshow(query_head_evolution, cmap=\"hot\", aspect=\"auto\", vmin=0, vmax=1)\n",
        "            ax.tick_params(left=False, bottom=False)\n",
        "            ax.set_xticks(range(seq_len))  # Set tick positions for 63 elements\n",
        "            ax.set_xticklabels([\"\"] * 32 + tokens + [\"[PAD]\"] * (63 - 32 - len(tokens)), rotation=45, ha=\"right\", fontsize = 8)\n",
        "            ax.set_yticks(range(num_layers))\n",
        "            ax.set_yticklabels([f\"{num_layers - i}\" for i in range(num_layers)])\n",
        "            if query_idx < 32:\n",
        "                ax.set_title(f\"Learnabla query {query_idx} - Head {head_idx+1}\", fontsize = 20)\n",
        "            else:\n",
        "                try:\n",
        "                    ax.set_title(f\"Query: {tokens[query_idx - 32]} - Avg over heads\", fontsize = 20)\n",
        "                except IndexError:\n",
        "                    ax.set_title(f\"Query: [PAD] - Avg over heads\", fontsize = 20)\n",
        "            ax.set_xlabel(\"Keys\", fontsize = 12)\n",
        "            ax.set_ylabel(\"Layers\", rotation=90, fontsize=12)\n",
        "\n",
        "\n",
        "            cbar = fig.colorbar(cax, ax=ax)\n",
        "            cbar.set_label(\"Attention Weight\")\n",
        "\n",
        "            # Save the image\n",
        "            fig.savefig(f\"{path_to_save}/query_{query_idx}_head_{head_idx+1}_evolution.pdf\", bbox_inches='tight', pad_inches=0, format= \"pdf\")\n",
        "            #fig.savefig(f\"{path_to_save}/query_{query_idx}_head_{head_idx+1}_evolution.png\")\n",
        "            plt.close(fig)\n",
        "\n",
        "        # Create an avg heatmap for all heads per query\n",
        "        query_avg_evolution = []\n",
        "        for layer_idx in range(num_layers):\n",
        "            avg_attention = torch.mean(rolled_out_attention[layer_idx, :, query_idx], dim=0).cpu().detach().numpy()\n",
        "            query_avg_evolution.insert(0, avg_attention)\n",
        "\n",
        "        query_avg_evolution = np.array(query_avg_evolution)  # [num_layers, seq_len]\n",
        "        # Print the query_avg_evolution for layer 12\n",
        "        #print(query_idx, query_avg_evolution[0])\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(12, 8))\n",
        "        cax = ax.imshow(query_avg_evolution, cmap=\"hot\", aspect=\"auto\", vmin=0, vmax=1)\n",
        "        ax.tick_params(left=False, bottom=False)\n",
        "        ax.set_xticks(range(seq_len))\n",
        "        ax.set_xticklabels([\"\"] * 32 + tokens + [\"[PAD]\"] * (63 - 32 - len(tokens)), rotation=45, ha=\"right\", fontsize = 8)\n",
        "        ax.set_yticks(range(num_layers))\n",
        "        ax.set_yticklabels([f\"{num_layers - i}\" for i in range(num_layers)])\n",
        "        if query_idx < 32:\n",
        "                ax.set_title(f\"Learnable query {query_idx} - Avg over heads\", fontsize=20)\n",
        "        else:\n",
        "            try:\n",
        "                ax.set_title(f\"Query: {tokens[query_idx - 32]} - Avg over heads\", fontsize=20)\n",
        "            except IndexError:\n",
        "                ax.set_title(f\"Query: [PAD] - Avg over heads\", fontsize=20)\n",
        "        ax.set_xlabel(\"Keys\", fontsize=12)\n",
        "        ax.set_ylabel(\"Layers\", fontsize=12)\n",
        "        #cbar = fig.colorbar(cax, ax=ax)\n",
        "        #cbar.set_label(\"Attention Weight\", fontsize=12)\n",
        "\n",
        "        # Save the avg heatmap per query\n",
        "        fig.savefig(f\"{path_to_save}/query_{query_idx}_average_evolution.pdf\", bbox_inches='tight', pad_inches=0, format='pdf')\n",
        "        #fig.savefig(f\"{path_to_save}/query_{query_idx}_average_evolution.png\",  bbox_inches='tight', pad_inches=0)\n",
        "        plt.close(fig)"
      ],
      "metadata": {
        "id": "AHYOy2sWC_aP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_samples = [0, 5, 17, 90]\n",
        "\n",
        "for sample in batch_samples:\n",
        "    plot_attention_rollout_per_query(concatenated_attention_outputs, BATCH_INDEX=0, BATCH_SAMPLE=sample)"
      ],
      "metadata": {
        "id": "C2VvUnE-EfCI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}